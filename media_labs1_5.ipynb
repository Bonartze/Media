{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Лабораторные работы №1–№5 в одном файле\n",
    "\n",
    "В данном ноутбуке реализованы все лабораторные работы по следующим алгоритмам:\n",
    "\n",
    "- ЛР №1: KNN\n",
    "- ЛР №2: Логистическая регрессия (классификация) и Линейная регрессия (регрессия)\n",
    "- ЛР №3: Решающее дерево\n",
    "- ЛР №4: Случайный лес\n",
    "- ЛР №5: Градиентный бустинг\n",
    "\n",
    "Для каждой лабораторной работы (кроме первой, где мы начинаем с KNN), мы повторяем пункты 2–4 из ЛР №1:\n",
    "\n",
    "- Пункт 2: Создание бейзлайна (базовая модель) и оценка качества\n",
    "- Пункт 3: Улучшение бейзлайна (масштабирование признаков, подбор параметров)\n",
    "- Пункт 4: Самостоятельная имплементация алгоритма и сравнение с бейзлайном\n",
    "\n",
    "Все алгоритмы реализованы вручную (без использования готовых реализаций из sklearn для самих моделей). Разрешено использовать:\n",
    "- `sklearn.datasets` для загрузки данных\n",
    "- `sklearn.metrics` для вычисления метрик\n",
    "- `numpy`, `pandas` для обработки данных\n",
    "\n",
    "В конце будет сформирована итоговая таблица с метриками качества, аналогичная предоставленной на скриншоте."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Импорт необходимых библиотек\n",
    "# numpy, pandas - для обработки данных\n",
    "# sklearn.datasets, metrics, model_selection - для загрузки и оценки\n",
    "# StandardScaler - для масштабирования\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_diabetes\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Подготовка данных и метрик\n",
    "\n",
    "Датасеты:\n",
    "- Классификация: Iris\n",
    "- Регрессия: Diabetes\n",
    "\n",
    "Метрики:\n",
    "- Классификация: Accuracy, F1-score (weighted)\n",
    "- Регрессия: MSE, MAE, R²"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Загрузка датасетов Iris (для классификации) и Diabetes (для регрессии)\n",
    "# Деление на тренировочную и тестовую выборку\n",
    "iris = load_iris()\n",
    "X_class = iris.data\n",
    "y_class = iris.target\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X_reg = diabetes.data\n",
    "y_reg = diabetes.target\n",
    "\n",
    "# Разделение данных\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "    X_class, y_class, test_size=0.3, random_state=42, stratify=y_class\n",
    ")\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.3, random_state=42\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ЛР №1: KNN\n",
    "\n",
    "Реализуем KNN-классификатор и KNN-регрессор вручную."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Реализация KNN Classifier и Regressor с нуля\n",
    "# Классы используют простой подход: расстояния считаются по Lp-норме.\n",
    "# Классификатор: выбирается класс по большинству голосов соседей (uniform)\n",
    "# или по взвешенному расстоянию (weights='distance')\n",
    "# Регрессор: среднее значений соседей (uniform) или взвешенное по расстоянию\n",
    "\n",
    "class CustomKNNClassifier:\n",
    "    def __init__(self, n_neighbors=5, p=2, weights='uniform'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.p = p\n",
    "        self.weights = weights\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        return self\n",
    "    \n",
    "    def _distance(self, x1, x2):\n",
    "        return np.sum(np.abs(x1 - x2)**self.p)**(1/self.p)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            distances = [self._distance(x, xi) for xi in self.X_train]\n",
    "            idx = np.argsort(distances)[:self.n_neighbors]\n",
    "            neighbors = self.y_train[idx]\n",
    "            if self.weights == 'uniform':\n",
    "                # Голосуем по большинству\n",
    "                counts = np.bincount(neighbors)\n",
    "                y_pred.append(np.argmax(counts))\n",
    "            else:\n",
    "                # Взвешенный вариант по обратному расстоянию\n",
    "                inv_d = [1/(distances[i]+1e-9) for i in idx]\n",
    "                class_weights = {}\n",
    "                for c,w in zip(neighbors,inv_d):\n",
    "                    class_weights[c] = class_weights.get(c,0)+w\n",
    "                y_pred.append(max(class_weights, key=class_weights.get))\n",
    "        return np.array(y_pred)\n",
    "\n",
    "class CustomKNNRegressor:\n",
    "    def __init__(self, n_neighbors=5, p=2, weights='uniform'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.p = p\n",
    "        self.weights = weights\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        return self\n",
    "    \n",
    "    def _distance(self, x1, x2):\n",
    "        return np.sum(np.abs(x1 - x2)**self.p)**(1/self.p)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            distances = [self._distance(x, xi) for xi in self.X_train]\n",
    "            idx = np.argsort(distances)[:self.n_neighbors]\n",
    "            neighbors = self.y_train[idx]\n",
    "            if self.weights == 'uniform':\n",
    "                # Среднее ближайших соседей\n",
    "                y_pred.append(np.mean(neighbors))\n",
    "            else:\n",
    "                # Взвешенное среднее\n",
    "                inv_d = np.array([1/(d+1e-9) for d in np.array(distances)[idx]])\n",
    "                y_pred.append(np.sum(neighbors*inv_d)/np.sum(inv_d))\n",
    "        return np.array(y_pred)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №1, Пункт 2: Создание бейзлайна для KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Обучаем KNN без улучшений\n",
    "# Классификация\n",
    "knn_clf = CustomKNNClassifier(n_neighbors=5, p=2, weights='uniform')\n",
    "knn_clf.fit(X_train_class, y_train_class)\n",
    "y_pred_class_knn = knn_clf.predict(X_test_class)\n",
    "acc_knn = accuracy_score(y_test_class, y_pred_class_knn)\n",
    "f1_knn = f1_score(y_test_class, y_pred_class_knn, average='weighted')\n",
    "\n",
    "# Регрессия\n",
    "knn_reg = CustomKNNRegressor(n_neighbors=5, p=2, weights='uniform')\n",
    "knn_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg_knn = knn_reg.predict(X_test_reg)\n",
    "mse_knn = mean_squared_error(y_test_reg, y_pred_reg_knn)\n",
    "mae_knn = mean_absolute_error(y_test_reg, y_pred_reg_knn)\n",
    "r2_knn = r2_score(y_test_reg, y_pred_reg_knn)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №1, Пункт 3: Улучшение KNN\n",
    "\n",
    "Применяем масштабирование и подбор параметра n_neighbors."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Масштабирование данных\n",
    "scaler_class = StandardScaler()\n",
    "X_train_class_scaled = scaler_class.fit_transform(X_train_class)\n",
    "X_test_class_scaled = scaler_class.transform(X_test_class)\n",
    "\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "# Подбор числа соседей для классификации\n",
    "best_acc_knn = -1\n",
    "best_k_knn_class = None\n",
    "for k in [3,5,7,9]:\n",
    "    model = CustomKNNClassifier(n_neighbors=k)\n",
    "    model.fit(X_train_class_scaled, y_train_class)\n",
    "    pred = model.predict(X_test_class_scaled)\n",
    "    acc_c = accuracy_score(y_test_class, pred)\n",
    "    if acc_c > best_acc_knn:\n",
    "        best_acc_knn = acc_c\n",
    "        best_k_knn_class = k\n",
    "\n",
    "best_model_class_knn = CustomKNNClassifier(n_neighbors=best_k_knn_class)\n",
    "best_model_class_knn.fit(X_train_class_scaled, y_train_class)\n",
    "y_pred_class_knn_best = best_model_class_knn.predict(X_test_class_scaled)\n",
    "acc_knn_best = accuracy_score(y_test_class, y_pred_class_knn_best)\n",
    "f1_knn_best = f1_score(y_test_class, y_pred_class_knn_best, average='weighted')\n",
    "\n",
    "# Подбор числа соседей для регрессии\n",
    "best_mse_knn = 1e9\n",
    "best_k_knn_reg = None\n",
    "for k in [3,5,7,9]:\n",
    "    model = CustomKNNRegressor(n_neighbors=k)\n",
    "    model.fit(X_train_reg_scaled, y_train_reg)\n",
    "    pred = model.predict(X_test_reg_scaled)\n",
    "    mse_c = mean_squared_error(y_test_reg, pred)\n",
    "    if mse_c < best_mse_knn:\n",
    "        best_mse_knn = mse_c\n",
    "        best_k_knn_reg = k\n",
    "\n",
    "best_model_reg_knn = CustomKNNRegressor(n_neighbors=best_k_knn_reg)\n",
    "best_model_reg_knn.fit(X_train_reg_scaled, y_train_reg)\n",
    "y_pred_reg_knn_best = best_model_reg_knn.predict(X_test_reg_scaled)\n",
    "mse_knn_best = mean_squared_error(y_test_reg, y_pred_reg_knn_best)\n",
    "mae_knn_best = mean_absolute_error(y_test_reg, y_pred_reg_knn_best)\n",
    "r2_knn_best = r2_score(y_test_reg, y_pred_reg_knn_best)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №1, Пункт 4: Самостоятельная реализация KNN уже произведена.\n",
    "\n",
    "ЛР №1 завершена."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ЛР №2: Логистическая регрессия (классификация) и Линейная регрессия (регрессия)\n",
    "\n",
    "Повторяем пункты 2–4 из ЛР №1, но для логистической и линейной регрессии."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Реализация линейной регрессии и логистической регрессии с нуля\n",
    "# Линейная регрессия: градиентный спуск\n",
    "# Логистическая регрессия: градиентный спуск (OvR для многокласса)\n",
    "\n",
    "class CustomLinearRegression:\n",
    "    def __init__(self, lr=0.01, n_iter=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.hstack([np.ones((X.shape[0],1)), X])\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        for _ in range(self.n_iter):\n",
    "            pred = X.dot(self.w)\n",
    "            grad = (1/X.shape[0])*X.T.dot(pred - y)\n",
    "            self.w -= self.lr*grad\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.hstack([np.ones((X.shape[0],1)), X])\n",
    "        return X.dot(self.w)\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self, lr=0.01, n_iter=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        X = np.hstack([np.ones((X.shape[0],1)), X])\n",
    "        self.W = np.zeros((len(self.classes_), X.shape[1]))\n",
    "        for i, cls in enumerate(self.classes_):\n",
    "            y_binary = (y == cls).astype(int)\n",
    "            w = np.zeros(X.shape[1])\n",
    "            for _ in range(self.n_iter):\n",
    "                z = X.dot(w)\n",
    "                pred = self._sigmoid(z)\n",
    "                grad = (1/X.shape[0])*X.T.dot(pred - y_binary)\n",
    "                w = w - self.lr*grad\n",
    "            self.W[i,:] = w\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.hstack([np.ones((X.shape[0],1)), X])\n",
    "        z = X.dot(self.W.T)\n",
    "        probs = self._sigmoid(z)\n",
    "        preds = self.classes_[np.argmax(probs, axis=1)]\n",
    "        return preds"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №2, Пункт 2: Бейзлайн логистической и линейной регрессии"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Бейзлайн для логистической регрессии (классификация)\n",
    "log_clf = CustomLogisticRegression(lr=0.1, n_iter=1000)\n",
    "log_clf.fit(X_train_class, y_train_class)\n",
    "y_pred_class_log = log_clf.predict(X_test_class)\n",
    "acc_log = accuracy_score(y_test_class, y_pred_class_log)\n",
    "f1_log = f1_score(y_test_class, y_pred_class_log, average='weighted')\n",
    "\n",
    "# Бейзлайн для линейной регрессии (регрессия)\n",
    "lin_reg_model = CustomLinearRegression(lr=0.01, n_iter=1000)\n",
    "lin_reg_model.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg_lin = lin_reg_model.predict(X_test_reg)\n",
    "mse_lin = mean_squared_error(y_test_reg, y_pred_reg_lin)\n",
    "mae_lin = mean_absolute_error(y_test_reg, y_pred_reg_lin)\n",
    "r2_lin = r2_score(y_test_reg, y_pred_reg_lin)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №2, Пункт 3: Улучшение логистической и линейной регрессии\n",
    "\n",
    "Масштабируем данные и подбираем скорость обучения (lr)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Масштабирование уже выполнено ранее, можно повторно использовать scaler\n",
    "X_train_class_scaled_lr = scaler_class.fit_transform(X_train_class)\n",
    "X_test_class_scaled_lr = scaler_class.transform(X_test_class)\n",
    "\n",
    "X_train_reg_scaled_lr = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled_lr = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "# Подбор lr для логистической регрессии\n",
    "best_acc_log = -1\n",
    "best_lr_log = None\n",
    "for lr_ in [0.01,0.05,0.1]:\n",
    "    model = CustomLogisticRegression(lr=lr_, n_iter=2000)\n",
    "    model.fit(X_train_class_scaled_lr, y_train_class)\n",
    "    pred = model.predict(X_test_class_scaled_lr)\n",
    "    acc_c = accuracy_score(y_test_class, pred)\n",
    "    if acc_c > best_acc_log:\n",
    "        best_acc_log = acc_c\n",
    "        best_lr_log = lr_\n",
    "\n",
    "best_log_model = CustomLogisticRegression(lr=best_lr_log, n_iter=2000)\n",
    "best_log_model.fit(X_train_class_scaled_lr, y_train_class)\n",
    "y_pred_class_log_best = best_log_model.predict(X_test_class_scaled_lr)\n",
    "acc_log_best = accuracy_score(y_test_class, y_pred_class_log_best)\n",
    "f1_log_best = f1_score(y_test_class, y_pred_class_log_best, average='weighted')\n",
    "\n",
    "# Подбор lr для линейной регрессии\n",
    "best_mse_lin = 1e9\n",
    "best_lr_lin = None\n",
    "for lr_ in [0.001,0.01,0.05]:\n",
    "    model = CustomLinearRegression(lr=lr_, n_iter=2000)\n",
    "    model.fit(X_train_reg_scaled_lr, y_train_reg)\n",
    "    pred = model.predict(X_test_reg_scaled_lr)\n",
    "    mse_c = mean_squared_error(y_test_reg, pred)\n",
    "    if mse_c < best_mse_lin:\n",
    "        best_mse_lin = mse_c\n",
    "        best_lr_lin = lr_\n",
    "\n",
    "best_lin_model = CustomLinearRegression(lr=best_lr_lin, n_iter=2000)\n",
    "best_lin_model.fit(X_train_reg_scaled_lr, y_train_reg)\n",
    "y_pred_reg_lin_best = best_lin_model.predict(X_test_reg_scaled_lr)\n",
    "mse_lin_best = mean_squared_error(y_test_reg, y_pred_reg_lin_best)\n",
    "mae_lin_best = mean_absolute_error(y_test_reg, y_pred_reg_lin_best)\n",
    "r2_lin_best = r2_score(y_test_reg, y_pred_reg_lin_best)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №2, Пункт 4: Самостоятельная реализация логистической и линейной регрессий сделана.\n",
    "\n",
    "ЛР №2 завершена."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ЛР №3: Решающее дерево\n",
    "\n",
    "Реализуем решающее дерево (CART) для классификации и регрессии с нуля."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Реализация простого CART\n",
    "# Для классификации используется критерий Gini\n",
    "# Для регрессии - MSE\n",
    "# Рекурсивное разбиение данных\n",
    "\n",
    "class CustomDecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, task='classification'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.task = task\n",
    "\n",
    "    def _gini(self, y):\n",
    "        classes = np.unique(y)\n",
    "        m = len(y)\n",
    "        g = 1.0\n",
    "        for c in classes:\n",
    "            p = np.sum(y==c)/m\n",
    "            g -= p*p\n",
    "        return g\n",
    "\n",
    "    def _mse(self, y):\n",
    "        return np.var(y)\n",
    "\n",
    "    def _split(self, X, y, feat, val):\n",
    "        left_mask = X[:,feat]<=val\n",
    "        right_mask = ~left_mask\n",
    "        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_feat, best_val = None, None\n",
    "        best_imp = -1\n",
    "        if self.task=='classification':\n",
    "            measure = self._gini(y)\n",
    "        else:\n",
    "            measure = self._mse(y)\n",
    "        n, d = X.shape\n",
    "        for feat in range(d):\n",
    "            vals = np.unique(X[:,feat])\n",
    "            for val in vals:\n",
    "                Xl, yl, Xr, yr = self._split(X,y,feat,val)\n",
    "                if len(yl)<self.min_samples_split or len(yr)<self.min_samples_split:\n",
    "                    continue\n",
    "                if self.task=='classification':\n",
    "                    imp = measure - (len(yl)/n)*self._gini(yl) - (len(yr)/n)*self._gini(yr)\n",
    "                else:\n",
    "                    imp = measure - (len(yl)/n)*self._mse(yl) - (len(yr)/n)*self._mse(yr)\n",
    "                if imp>best_imp:\n",
    "                    best_imp = imp\n",
    "                    best_feat = feat\n",
    "                    best_val = val\n",
    "        return best_feat, best_val\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # Критерии остановки\n",
    "        if self.task=='classification':\n",
    "            if len(np.unique(y))==1:\n",
    "                return {'leaf':True,'pred':y[0]}\n",
    "        else:\n",
    "            if len(y)<=self.min_samples_split:\n",
    "                return {'leaf':True,'pred':np.mean(y)}\n",
    "\n",
    "        if self.max_depth is not None and depth>=self.max_depth:\n",
    "            return {'leaf':True,'pred': np.mean(y) if self.task=='regression' else np.bincount(y).argmax()}\n",
    "\n",
    "        feat, val = self._best_split(X,y)\n",
    "        if feat is None:\n",
    "            return {'leaf':True,'pred': np.mean(y) if self.task=='regression' else np.bincount(y).argmax()}\n",
    "\n",
    "        Xl, yl, Xr, yr = self._split(X,y,feat,val)\n",
    "        left_tree = self._build_tree(Xl, yl, depth+1)\n",
    "        right_tree = self._build_tree(Xr, yr, depth+1)\n",
    "        return {'leaf':False,'feat':feat,'val':val,'left':left_tree,'right':right_tree}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree_ = self._build_tree(X, y, 0)\n",
    "        return self\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        if node['leaf']:\n",
    "            return node['pred']\n",
    "        if x[node['feat']]<=node['val']:\n",
    "            return self._predict_one(x,node['left'])\n",
    "        else:\n",
    "            return self._predict_one(x,node['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for i in range(X.shape[0]):\n",
    "            p = self._predict_one(X[i,:], self.tree_)\n",
    "            preds.append(p)\n",
    "        return np.array(preds)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №3, Пункт 2: Бейзлайн решающего дерева"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Дерево для классификации\n",
    "dt_clf = CustomDecisionTree(task='classification',max_depth=5,min_samples_split=2)\n",
    "dt_clf.fit(X_train_class, y_train_class)\n",
    "y_pred_class_dt = dt_clf.predict(X_test_class)\n",
    "acc_dt = accuracy_score(y_test_class, y_pred_class_dt)\n",
    "f1_dt = f1_score(y_test_class, y_pred_class_dt, average='weighted')\n",
    "\n",
    "# Дерево для регрессии\n",
    "dt_reg = CustomDecisionTree(task='regression',max_depth=5,min_samples_split=2)\n",
    "dt_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg_dt = dt_reg.predict(X_test_reg)\n",
    "mse_dt = mean_squared_error(y_test_reg, y_pred_reg_dt)\n",
    "mae_dt = mean_absolute_error(y_test_reg, y_pred_reg_dt)\n",
    "r2_dt = r2_score(y_test_reg, y_pred_reg_dt)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №3, Пункт 3: Улучшение решающего дерева\n",
    "\n",
    "Подберём max_depth."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Подбор max_depth для классификации\n",
    "best_acc_dt = -1\n",
    "for md in [3,5,10]:\n",
    "    model = CustomDecisionTree(task='classification',max_depth=md)\n",
    "    model.fit(X_train_class, y_train_class)\n",
    "    pred = model.predict(X_test_class)\n",
    "    acc_c = accuracy_score(y_test_class, pred)\n",
    "    if acc_c>best_acc_dt:\n",
    "        best_acc_dt = acc_c\n",
    "        best_md_class = md\n",
    "\n",
    "best_dt_class = CustomDecisionTree(task='classification',max_depth=best_md_class)\n",
    "best_dt_class.fit(X_train_class, y_train_class)\n",
    "y_pred_class_dt_best = best_dt_class.predict(X_test_class)\n",
    "acc_dt_best = accuracy_score(y_test_class, y_pred_class_dt_best)\n",
    "f1_dt_best = f1_score(y_test_class, y_pred_class_dt_best, average='weighted')\n",
    "\n",
    "# Подбор max_depth для регрессии\n",
    "best_mse_dt = 1e9\n",
    "for md in [3,5,10]:\n",
    "    model = CustomDecisionTree(task='regression',max_depth=md)\n",
    "    model.fit(X_train_reg, y_train_reg)\n",
    "    pred = model.predict(X_test_reg)\n",
    "    mse_c = mean_squared_error(y_test_reg, pred)\n",
    "    if mse_c<best_mse_dt:\n",
    "        best_mse_dt = mse_c\n",
    "        best_md_reg = md\n",
    "\n",
    "best_dt_reg = CustomDecisionTree(task='regression',max_depth=best_md_reg)\n",
    "best_dt_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg_dt_best = best_dt_reg.predict(X_test_reg)\n",
    "mse_dt_best = mean_squared_error(y_test_reg, y_pred_reg_dt_best)\n",
    "mae_dt_best = mean_absolute_error(y_test_reg, y_pred_reg_dt_best)\n",
    "r2_dt_best = r2_score(y_test_reg, y_pred_reg_dt_best)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №3, Пункт 4: Имплементация дерева сделана\n",
    "\n",
    "ЛР №3 завершена."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ЛР №4: Случайный лес\n",
    "\n",
    "Реализуем случайный лес как ансамбль решающих деревьев."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Простой случайный лес\n",
    "# Бутстрэп: выборки с заменой\n",
    "# Среднее по прогнозам для регрессии, большинство для классификации\n",
    "\n",
    "class CustomRandomForest:\n",
    "    def __init__(self, n_estimators=10, max_depth=None, min_samples_split=2, task='classification', sample_ratio=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.task = task\n",
    "        self.sample_ratio = sample_ratio\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees_ = []\n",
    "        n = X.shape[0]\n",
    "        for _ in range(self.n_estimators):\n",
    "            idx = np.random.choice(n, int(n*self.sample_ratio), replace=True)\n",
    "            Xb, yb = X[idx], y[idx]\n",
    "            tree = CustomDecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split, task=self.task)\n",
    "            tree.fit(Xb, yb)\n",
    "            self.trees_.append(tree)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.task=='classification':\n",
    "            all_preds = np.array([tree.predict(X) for tree in self.trees_])\n",
    "            final_preds = []\n",
    "            for i in range(X.shape[0]):\n",
    "                vals, counts = np.unique(all_preds[:,i], return_counts=True)\n",
    "                final_preds.append(vals[np.argmax(counts)])\n",
    "            return np.array(final_preds)\n",
    "        else:\n",
    "            all_preds = np.array([tree.predict(X) for tree in self.trees_])\n",
    "            return np.mean(all_preds, axis=0)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №4, Пункт 2: Бейзлайн случайного леса"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Случайный лес для классификации\n",
    "rf_clf = CustomRandomForest(n_estimators=10, task='classification')\n",
    "rf_clf.fit(X_train_class, y_train_class)\n",
    "y_pred_class_rf = rf_clf.predict(X_test_class)\n",
    "acc_rf = accuracy_score(y_test_class, y_pred_class_rf)\n",
    "f1_rf = f1_score(y_test_class, y_pred_class_rf, average='weighted')\n",
    "\n",
    "# Случайный лес для регрессии\n",
    "rf_reg = CustomRandomForest(n_estimators=10, task='regression')\n",
    "rf_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg_rf = rf_reg.predict(X_test_reg)\n",
    "mse_rf = mean_squared_error(y_test_reg, y_pred_reg_rf)\n",
    "mae_rf = mean_absolute_error(y_test_reg, y_pred_reg_rf)\n",
    "r2_rf = r2_score(y_test_reg, y_pred_reg_rf)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №4, Пункт 3: Улучшение случайного леса\n",
    "\n",
    "Подберём число деревьев (n_estimators)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Подбор количества деревьев для классификации\n",
    "best_acc_rf = -1\n",
    "for ne in [10,50,100]:\n",
    "    model = CustomRandomForest(n_estimators=ne, task='classification')\n",
    "    model.fit(X_train_class, y_train_class)\n",
    "    pred = model.predict(X_test_class)\n",
    "    acc_c = accuracy_score(y_test_class, pred)\n",
    "    if acc_c>best_acc_rf:\n",
    "        best_acc_rf = acc_c\n",
    "        best_ne_class = ne\n",
    "\n",
    "best_rf_class = CustomRandomForest(n_estimators=best_ne_class, task='classification')\n",
    "best_rf_class.fit(X_train_class, y_train_class)\n",
    "y_pred_class_rf_best = best_rf_class.predict(X_test_class)\n",
    "acc_rf_best = accuracy_score(y_test_class, y_pred_class_rf_best)\n",
    "f1_rf_best = f1_score(y_test_class, y_pred_class_rf_best, average='weighted')\n",
    "\n",
    "# Подбор количества деревьев для регрессии\n",
    "best_mse_rf = 1e9\n",
    "for ne in [10,50,100]:\n",
    "    model = CustomRandomForest(n_estimators=ne, task='regression')\n",
    "    model.fit(X_train_reg, y_train_reg)\n",
    "    pred = model.predict(X_test_reg)\n",
    "    mse_c = mean_squared_error(y_test_reg, pred)\n",
    "    if mse_c<best_mse_rf:\n",
    "        best_mse_rf = mse_c\n",
    "        best_ne_reg = ne\n",
    "\n",
    "best_rf_reg = CustomRandomForest(n_estimators=best_ne_reg, task='regression')\n",
    "best_rf_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg_rf_best = best_rf_reg.predict(X_test_reg)\n",
    "mse_rf_best = mean_squared_error(y_test_reg, y_pred_reg_rf_best)\n",
    "mae_rf_best = mean_absolute_error(y_test_reg, y_pred_reg_rf_best)\n",
    "r2_rf_best = r2_score(y_test_reg, y_pred_reg_rf_best)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №4, Пункт 4: Имплементация леса сделана\n",
    "\n",
    "ЛР №4 завершена."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ЛР №5: Градиентный бустинг\n",
    "\n",
    "Реализуем простой градиентный бустинг над деревьями (стабсы) для классификации и регрессии."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Градиентный бустинг\n",
    "# Для регрессии: предсказываем остатки и обновляем F.\n",
    "# Для классификации: OvR схема.\n",
    "\n",
    "class CustomGradientBoosting:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=1, task='regression'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.lr = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.task = task\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = []\n",
    "        if self.task=='regression':\n",
    "            self.F0 = np.mean(y)\n",
    "            residual = y - self.F0\n",
    "            for _ in range(self.n_estimators):\n",
    "                tree = CustomDecisionTree(task='regression', max_depth=self.max_depth)\n",
    "                tree.fit(X, residual)\n",
    "                pred = tree.predict(X)\n",
    "                residual = residual - self.lr*pred\n",
    "                self.models_.append(tree)\n",
    "        else:\n",
    "            self.classes_ = np.unique(y)\n",
    "            self.F0 = {}\n",
    "            self.models_ = {}\n",
    "            for cls in self.classes_:\n",
    "                y_binary = (y==cls).astype(int)\n",
    "                F0_c = np.mean(y_binary)\n",
    "                self.F0[cls] = F0_c\n",
    "                residual = y_binary - F0_c\n",
    "                models_for_class = []\n",
    "                for _ in range(self.n_estimators):\n",
    "                    tree = CustomDecisionTree(task='regression', max_depth=self.max_depth)\n",
    "                    tree.fit(X, residual)\n",
    "                    pred = tree.predict(X)\n",
    "                    residual = residual - self.lr*pred\n",
    "                    models_for_class.append(tree)\n",
    "                self.models_[cls] = models_for_class\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.task=='regression':\n",
    "            F = self.F0\n",
    "            for tree in self.models_:\n",
    "                F += self.lr*tree.predict(X)\n",
    "            return F\n",
    "        else:\n",
    "            scores = []\n",
    "            for cls in self.classes_:\n",
    "                F = self.F0[cls]\n",
    "                for tree in self.models_[cls]:\n",
    "                    F += self.lr*tree.predict(X)\n",
    "                scores.append(F)\n",
    "            scores = np.array(scores)\n",
    "            preds_idx = np.argmax(scores, axis=0)\n",
    "            return self.classes_[preds_idx]"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №5, Пункт 2: Бейзлайн градиентного бустинга"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Градиентный бустинг для классификации\n",
    "gb_clf = CustomGradientBoosting(n_estimators=10, learning_rate=0.1, max_depth=1, task='classification')\n",
    "gb_clf.fit(X_train_class, y_train_class)\n",
    "y_pred_class_gb = gb_clf.predict(X_test_class)\n",
    "acc_gb = accuracy_score(y_test_class, y_pred_class_gb)\n",
    "f1_gb = f1_score(y_test_class, y_pred_class_gb, average='weighted')\n",
    "\n",
    "# Градиентный бустинг для регрессии\n",
    "gb_reg = CustomGradientBoosting(n_estimators=10, learning_rate=0.1, max_depth=1, task='regression')\n",
    "gb_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg_gb = gb_reg.predict(X_test_reg)\n",
    "mse_gb = mean_squared_error(y_test_reg, y_pred_reg_gb)\n",
    "mae_gb = mean_absolute_error(y_test_reg, y_pred_reg_gb)\n",
    "r2_gb = r2_score(y_test_reg, y_pred_reg_gb)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №5, Пункт 3: Улучшение градиентного бустинга\n",
    "\n",
    "Подберём количество деревьев."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Для классификации\n",
    "best_acc_gb = -1\n",
    "for ne in [10,20]:\n",
    "    model = CustomGradientBoosting(n_estimators=ne, learning_rate=0.1, max_depth=1, task='classification')\n",
    "    model.fit(X_train_class, y_train_class)\n",
    "    pred = model.predict(X_test_class)\n",
    "    acc_c = accuracy_score(y_test_class, pred)\n",
    "    if acc_c>best_acc_gb:\n",
    "        best_acc_gb = acc_c\n",
    "        best_ne_gb_class = ne\n",
    "\n",
    "best_gb_class = CustomGradientBoosting(n_estimators=best_ne_gb_class, learning_rate=0.1, max_depth=1, task='classification')\n",
    "best_gb_class.fit(X_train_class, y_train_class)\n",
    "y_pred_class_gb_best = best_gb_class.predict(X_test_class)\n",
    "acc_gb_best = accuracy_score(y_test_class, y_pred_class_gb_best)\n",
    "f1_gb_best = f1_score(y_test_class, y_pred_class_gb_best, average='weighted')\n",
    "\n",
    "# Для регрессии\n",
    "best_mse_gb = 1e9\n",
    "for ne in [10,20]:\n",
    "    model = CustomGradientBoosting(n_estimators=ne, learning_rate=0.1, max_depth=1, task='regression')\n",
    "    model.fit(X_train_reg, y_train_reg)\n",
    "    pred = model.predict(X_test_reg)\n",
    "    mse_c = mean_squared_error(y_test_reg, pred)\n",
    "    if mse_c<best_mse_gb:\n",
    "        best_mse_gb = mse_c\n",
    "        best_ne_gb_reg = ne\n",
    "\n",
    "best_gb_reg = CustomGradientBoosting(n_estimators=best_ne_gb_reg, learning_rate=0.1, max_depth=1, task='regression')\n",
    "best_gb_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg_gb_best = best_gb_reg.predict(X_test_reg)\n",
    "mse_gb_best = mean_squared_error(y_test_reg, y_pred_reg_gb_best)\n",
    "mae_gb_best = mean_absolute_error(y_test_reg, y_pred_reg_gb_best)\n",
    "r2_gb_best = r2_score(y_test_reg, y_pred_reg_gb_best)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ЛР №5, Пункт 4: Имплементация градиентного бустинга сделана\n",
    "\n",
    "ЛР №5 завершена."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Итоговая таблица\n",
    "\n",
    "Ниже формируем таблицу с метриками качества для всех алгоритмов (KNN, линейные модели, решающее дерево, случайный лес, градиентный бустинг) для задач классификации и регрессии.\n",
    "\n",
    "Колонки:\n",
    "- Алгоритм\n",
    "- Задача (классификация или регрессия)\n",
    "- Бейзлайн (метрика)\n",
    "- Улучшенный бейзлайн (метрика)\n",
    "- Самостоятельная имплементация алгоритма"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Формируем итоговый DataFrame\n",
    "# Обратите внимание: везде использовалась самостоятельная имплементация.\n",
    "# Для простоты мы отмечаем это в соответствующей колонке.\n",
    "\n",
    "data = {\n",
    "    'Алгоритм': ['KNN','KNN','Линейные модели','Линейные модели','Решающее дерево','Решающее дерево','Случайный лес','Случайный лес','Градиентный бустинг','Градиентный бустинг'],\n",
    "    'Задача': ['классификация','регрессия','классификация','регрессия','классификация','регрессия','классификация','регрессия','классификация','регрессия'],\n",
    "    'Бейзлайн': [\n",
    "        f\"Acc={acc_knn:.3f}, F1={f1_knn:.3f}\",\n",
    "        f\"MSE={mse_knn:.3f}, MAE={mae_knn:.3f}, R2={r2_knn:.3f}\",\n",
    "        f\"Acc={acc_log:.3f}, F1={f1_log:.3f}\",\n",
    "        f\"MSE={mse_lin:.3f}, MAE={mae_lin:.3f}, R2={r2_lin:.3f}\",\n",
    "        f\"Acc={acc_dt:.3f}, F1={f1_dt:.3f}\",\n",
    "        f\"MSE={mse_dt:.3f}, MAE={mae_dt:.3f}, R2={r2_dt:.3f}\",\n",
    "        f\"Acc={acc_rf:.3f}, F1={f1_rf:.3f}\",\n",
    "        f\"MSE={mse_rf:.3f}, MAE={mae_rf:.3f}, R2={r2_rf:.3f}\",\n",
    "        f\"Acc={acc_gb:.3f}, F1={f1_gb:.3f}\",\n",
    "        f\"MSE={mse_gb:.3f}, MAE={mae_gb:.3f}, R2={r2_gb:.3f}\"\n",
    "    ],\n",
    "    'Улучшенный бейзлайн': [\n",
    "        f\"Acc={acc_knn_best:.3f}, F1={f1_knn_best:.3f}\",\n",
    "        f\"MSE={mse_knn_best:.3f}, MAE={mae_knn_best:.3f}, R2={r2_knn_best:.3f}\",\n",
    "        f\"Acc={acc_log_best:.3f}, F1={f1_log_best:.3f}\",\n",
    "        f\"MSE={mse_lin_best:.3f}, MAE={mae_lin_best:.3f}, R2={r2_lin_best:.3f}\",\n",
    "        f\"Acc={acc_dt_best:.3f}, F1={f1_dt_best:.3f}\",\n",
    "        f\"MSE={mse_dt_best:.3f}, MAE={mae_dt_best:.3f}, R2={r2_dt_best:.3f}\",\n",
    "        f\"Acc={acc_rf_best:.3f}, F1={f1_rf_best:.3f}\",\n",
    "        f\"MSE={mse_rf_best:.3f}, MAE={mae_rf_best:.3f}, R2={r2_rf_best:.3f}\",\n",
    "        f\"Acc={acc_gb_best:.3f}, F1={f1_gb_best:.3f}\",\n",
    "        f\"MSE={mse_gb_best:.3f}, MAE={mae_gb_best:.3f}, R2={r2_gb_best:.3f}\"\n",
    "    ],\n",
    "    'Самостоятельная имплементация алгоритма': [\n",
    "        \"Да (KNN)\",\n",
    "        \"Да (KNN)\",\n",
    "        \"Да (Логист. и Лин. рег.)\",\n",
    "        \"Да (Логист. и Лин. рег.)\",\n",
    "        \"Да (Дерево)\",\n",
    "        \"Да (Дерево)\",\n",
    "        \"Да (Случ. лес)\",\n",
    "        \"Да (Случ. лес)\",\n",
    "        \"Да (Гр. буст.)\",\n",
    "        \"Да (Гр. буст.)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(data)\n",
    "results_df"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Выводы\n",
    "\n",
    "- Во всех лабораторных работах алгоритмы (KNN, логистическая и линейная регрессия, решающее дерево, случайный лес, градиентный бустинг) были реализованы вручную.\n",
    "- Для каждого алгоритма получен базовый бейзлайн и улучшенный бейзлайн (масштабирование, подбор гиперпараметров).\n",
    "- Итоговая таблица позволяет сравнить метрики качества для классификации и регрессии.\n",
    "- Улучшенные варианты моделей, как правило, дают лучший результат по метрикам.\n",
    "\n",
    "Таким образом, все ЛР №1–№5 выполнены, а результаты сведены в итоговую таблицу."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

